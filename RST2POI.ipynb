{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-06 13:09:22.717331: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-06 13:09:23.980709: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-06 13:09:26.908342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2023-06-06 13:09:26.909094: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64\n",
      "2023-06-06 13:09:26.909104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_p_path = f\"out/gijon/restaurants/reviews.pkl\"\n",
    "reviews = pd.read_pickle(reviews_p_path)\n",
    "reviews.groupby(\"itemId\")[\"itemId\"].count().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barcelona (RST) -> Paris (POI)\n",
    "# NewYork (RST) -> Paris (POI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener los ejemplos para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(city_r, city_p):\n",
    "    # Obtener el conjunto de datos para entrenar (comieron en r y fueron a p)\n",
    "    top_n_pois = 50\n",
    "\n",
    "    city_r = city_r.lower().replace(\" \", \"\")\n",
    "    city_p = city_p.lower().replace(\" \", \"\")\n",
    "\n",
    "    reviews_r_path = f\"out/{city_r}/restaurants/reviews.pkl\"\n",
    "    reviews_p_path = f\"out/{city_p}/pois/reviews.pkl\"\n",
    "\n",
    "    reviews_r = pd.read_pickle(reviews_r_path)\n",
    "    reviews_p = pd.read_pickle(reviews_p_path)\n",
    "\n",
    "    reviews_r = reviews_r[reviews_r[\"userId\"]!=-1]\n",
    "    reviews_p = reviews_p[reviews_p[\"userId\"]!=-1]\n",
    "\n",
    "    # Quedarse con los POIs más populares\n",
    "    reviews_p_popular = reviews_p.groupby(\"itemId\")[\"itemId\"].count().sort_values(ascending=False).head(top_n_pois).index.values\n",
    "    reviews_p = reviews_p[reviews_p[\"itemId\"].isin(reviews_p_popular)]\n",
    "\n",
    "    r_data = set(reviews_r[\"userId\"].unique())\n",
    "    p_data = set(reviews_p[\"userId\"].unique())\n",
    "\n",
    "    common_users = r_data.intersection(p_data)\n",
    "\n",
    "    # ToDo: ¿Por que hay menos usuarios en el conjunto users.pkl que en reviews.pkl?\n",
    "    # ToDo: En R, tienen que ser solo los comunes???\n",
    "    reviews_r = reviews_r.loc[reviews_r[\"userId\"].isin(common_users)]\n",
    "    reviews_p = reviews_p.loc[reviews_p[\"userId\"].isin(common_users)]\n",
    "\n",
    "    out_data = []\n",
    "\n",
    "    for rst_id, rst_data in reviews_r.groupby(\"itemId\"):\n",
    "        rst_users = rst_data[\"userId\"].unique()\n",
    "        poi_revws = reviews_p.loc[reviews_p[\"userId\"].isin(rst_users)]\n",
    "        poi_revws = poi_revws.groupby(\"userId\")[\"itemId\"].unique().reset_index()\n",
    "        # ToDo: Ojo, que los usuarios de r van a más de un POI en p\n",
    "        poiId, times = np.unique(np.concatenate(poi_revws[\"itemId\"].values), return_counts=True)\n",
    "\n",
    "        print(rst_id, len(rst_users), poiId )\n",
    "\n",
    "    return reviews_r, reviews_p\n",
    "\n",
    "dataset = get_data(\"barcelona\", \"paris\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formas de codificar un restaurante por sus fotos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        itemId                        name  n_images  n_reviews   \n",
      "0       693801                   Bar Celta       203        107  \\\n",
      "1       693822          Restaurante Manolo        18         10   \n",
      "2       693855                   Can Lluis        80         35   \n",
      "3       693967                   Elisabets       316        143   \n",
      "4       697396                    7 Portes      2676       1172   \n",
      "...        ...                         ...       ...        ...   \n",
      "6509  25338856             Velvet Room BCN         4          1   \n",
      "6510  25361641       MiMi Tapas Restaurant         6          4   \n",
      "6511  25362471             Mimi Restaurant         7          3   \n",
      "6512  25363246           Fonda Can Portell         4          1   \n",
      "6513  25386551  McDonalds - Som Multiespai         4          1   \n",
      "\n",
      "                                          item_encoding  \n",
      "0     [0.0011015198, -0.045368917, 0.06882931, -0.06...  \n",
      "1     [-0.08390628, 0.0139746815, 0.18736298, -0.102...  \n",
      "2     [-0.049717955, -0.06934771, 0.1684383, -0.0441...  \n",
      "3     [-0.037663653, -0.07932761, 0.13506116, -0.127...  \n",
      "4     [-0.06365619, -0.0982724, 0.16331768, -0.06829...  \n",
      "...                                                 ...  \n",
      "6509  [-0.13259922, 0.07872795, 0.2050268, -0.180780...  \n",
      "6510  [-0.11273998, -0.046104856, 0.22787046, -0.168...  \n",
      "6511  [-0.009020974, 0.010424785, 0.006621013, 0.339...  \n",
      "6512  [-0.071157925, -0.10712695, -0.091845095, 0.06...  \n",
      "6513  [-0.24776119, 0.18385397, -0.068481326, 0.0473...  \n",
      "\n",
      "[6514 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "def read_image(path):\n",
    "    try:\n",
    "        # Carga la imagen desde el path.\n",
    "        img = cv2.imread(path)\n",
    "        # BGR a RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # Normaliza la imagen entre -1 y 1.\n",
    "        img = (img / 127.5) - 1\n",
    "        # Redimensiona la imagen a 150x150 píxeles.\n",
    "        img = cv2.resize(img, (150, 150))\n",
    "        # Agrega una dimensión adicional para el batch.\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "    except Exception as e:\n",
    "        print(path)\n",
    "        print(e)\n",
    "    return img\n",
    "\n",
    "def encode_items(city, method=\"ImageNet\", category=\"restaurants\", min_images=4):\n",
    "    \"\"\"Codificar cada item haciendo una media de sus imágenes\"\"\"\n",
    "    out_file = f\"{city}_itm_emb.pkl\"\n",
    "\n",
    "    if not os.path.exists(out_file):\n",
    "        # Cargar datos\n",
    "        city = city.lower().replace(\" \", \"\")\n",
    "        city_path = f\"out/{city}/{category}\"\n",
    "        items_path = f\"{city_path}/items.pkl\"\n",
    "        reviews_path = f\"{city_path}/reviews.pkl\" # OJO QUE HAY DUPLICADOS\n",
    "        items = pd.read_pickle(items_path)\n",
    "        reviews = pd.read_pickle(reviews_path)\n",
    "\n",
    "        # Combinar reviews e items\n",
    "        reviews = reviews.merge(items[[\"itemId\", \"name\"]], on=\"itemId\", how=\"left\")\n",
    "        reviews[\"n_images\"] = reviews[\"images\"].apply(lambda x: len(x))\n",
    "\n",
    "        # Solo items con imágenes\n",
    "        reviews = reviews.loc[reviews[\"n_images\"]>0]\n",
    "\n",
    "        # Seleccionar el encoder\n",
    "        encoder = None\n",
    "        if method == \"ImageNet\":\n",
    "            encoder = tf.keras.Sequential([hub.KerasLayer(\"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_m/feature_vector/2\", trainable=False)])\n",
    "            encoder.build([None, 150, 150, 3]) # Batch input shape.\n",
    "        elif method == \"CLIP\": \n",
    "            encoder = None\n",
    "        else: \n",
    "            raise ValueError # Añadir SemPic??\n",
    "\n",
    "        # Crear, para cada item, los vectores a partir de sus imágenes\n",
    "        res = []\n",
    "        for iid, idata in tqdm(reviews.groupby(\"itemId\")):\n",
    "            iname = idata[\"name\"].unique()[0]\n",
    "\n",
    "            if idata[\"n_images\"].sum()<min_images: continue\n",
    "\n",
    "            # Quedarse solo con reviews con imágenes y explotar los vectores de imágenes\n",
    "            idata_images = idata.explode(\"images\").drop_duplicates(\"reviewId\")\n",
    "            idata_images[\"image_id\"] = idata_images.groupby([\"itemId\", \"reviewId\"]).cumcount()\n",
    "            idata_images = idata_images.rename(columns={\"images\": \"image_url\", \"image\": \"image\"})\n",
    "\n",
    "            idata_image_paths = idata_images.apply(lambda x: f'{city_path}/images/sd/{iid}/{x[\"reviewId\"]}/{x[\"image_id\"]:04d}.jpg',1).values\n",
    "\n",
    "            img_mtx = []\n",
    "            for path in idata_image_paths:\n",
    "                img_data = read_image(path)\n",
    "                img_mtx.append(img_data)\n",
    "            img_mtx = np.concatenate(img_mtx)\n",
    "            encodings = encoder.predict(img_mtx, verbose=0)\n",
    "            ienc = np.mean(encodings, 0)\n",
    "\n",
    "            res.append((iid, iname,  idata[\"n_images\"].sum(), len(idata), ienc))\n",
    "        res = pd.DataFrame(res, columns=[\"itemId\", \"name\", \"n_images\", \"n_reviews\", \"item_encoding\"])\n",
    "        res.to_pickle(f\"{city}_itm_emb.pkl\")\n",
    "    else:\n",
    "        res = pd.read_pickle(out_file)\n",
    "    return res\n",
    "\n",
    "dataset = encode_items(\"barcelona\")\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE de restaurantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "data_2d = tsne.fit_transform(np.array(dataset['item_encoding'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LinearColorMapper\n",
    "from bokeh.models import ColumnDataSource\n",
    "#from bokeh.plotting import figure, show\n",
    "#from bokeh.resources import INLINE\n",
    "\n",
    "#output_notebook(INLINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_2d' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m source \u001b[39m=\u001b[39m ColumnDataSource(data\u001b[39m=\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[0;32m----> 2\u001b[0m     x\u001b[39m=\u001b[39mdata_2d[:,\u001b[39m0\u001b[39m],\n\u001b[1;32m      3\u001b[0m     y\u001b[39m=\u001b[39mdata_2d[:,\u001b[39m1\u001b[39m],\n\u001b[1;32m      4\u001b[0m     name\u001b[39m=\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     color\u001b[39m=\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39mn_images\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m ))\n\u001b[1;32m      8\u001b[0m TOOLTIPS \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mName\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m@name\u001b[39m\u001b[39m\"\u001b[39m),(\u001b[39m\"\u001b[39m\u001b[39mn_images\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m@color\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m      9\u001b[0m p \u001b[39m=\u001b[39m figure(width\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, tooltips\u001b[39m=\u001b[39mTOOLTIPS)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_2d' is not defined"
     ]
    }
   ],
   "source": [
    "source = ColumnDataSource(data=dict(\n",
    "    x=data_2d[:,0],\n",
    "    y=data_2d[:,1],\n",
    "    name=dataset['name'],\n",
    "    color=dataset['n_images']\n",
    "))\n",
    "\n",
    "TOOLTIPS = [(\"Name\", \"@name\"),(\"n_images\", \"@color\")]\n",
    "p = figure(width=1000, height=1000, tooltips=TOOLTIPS)\n",
    "\n",
    "lc = LinearColorMapper(palette=\"Greys256\", low=dataset['n_images'].max(), high=dataset['n_images'].min())\n",
    "p.circle('x', 'y', source=source, size=10, fill_color={\"field\": \"color\", \"transform\": lc})\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TripAdvisorDownload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
